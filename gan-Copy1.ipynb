{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "933cc5f3",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3299bea",
   "metadata": {},
   "source": [
    "This notebook will explore using GAN to to turn regular images into those with a Monet style of painting. The data consists of 300 monet paintings to learn from and 7000 images to generate into fake money style images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7499dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of replicas: 1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Device:', tpu.master())\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "print('Number of replicas:', strategy.num_replicas_in_sync)\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca421141",
   "metadata": {},
   "outputs": [],
   "source": [
    "monet_filenames = os.listdir(r'C:\\Users\\Vedhik\\Documents\\Master of Data Science\\Intro to deep learning\\gan\\monet_tfrec')\n",
    "photo_filenames = os.listdir(r'C:\\Users\\Vedhik\\Documents\\Master of Data Science\\Intro to deep learning\\gan\\photo_tfrec')\n",
    "\n",
    "def count_data_items(filenames):\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)\n",
    "\n",
    "n_monet_samples = count_data_items(monet_filenames)\n",
    "n_photo_samples = count_data_items(photo_filenames)\n",
    "\n",
    "BATCH_SIZE =  4\n",
    "EPOCHS_NUM = 30\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6df30629",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [256, 256]\n",
    "def decode_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = (tf.cast(image, tf.float32) / 127.5) - 1\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
    "    return image\n",
    "\n",
    "def read_tfrecord(example):\n",
    "    tfrecord_format = {\n",
    "        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
    "    image = decode_image(example['image'])\n",
    "    return image\n",
    "\n",
    "def load_dataset(filenames, labeled=True, ordered=False):\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95c4ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "monet_ds = load_dataset(monet_filenames, labeled=True).batch(1)\n",
    "photo_ds = load_dataset(photo_filenames, labeled=True).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70b6eb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3\n",
    "\n",
    "\n",
    "def downsample(filters, size, normalize=True):\n",
    "    intializer = tf.random_normal_initializer(0., .02)\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(keras.layers.Conv2D(filters, size, strides=2, padding='same', kernel_initializer=intializer, use_bias=False))\n",
    "    if normalize:\n",
    "        model.add(tfa.layers.InstanceNormalization(gamma_initializer=intializer))\n",
    "    \n",
    "    model.add(keras.layers.LeakyReLU(alpha=0.2))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f69bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsampling(filters, size, dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., .02)\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(\n",
    "    keras.layers.Conv2DTranspose(filters, size, strides=2,padding='same', kernel_initializer=initializer,use_bias=False))\n",
    "\n",
    "    model.add(tfa.layers.InstanceNormalization(gamma_initializer=initializer))\n",
    "    if dropout:\n",
    "        model.add(keras.layers.Dropout(.5))\n",
    "\n",
    "    model.add(keras.layers.ReLU())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0e281b",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59a1713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator():\n",
    "    inputs = keras.layers.Input(shape=[256, 256, 3])\n",
    "    down_sampling = [\n",
    "        downsample(64, 4, normalize=False),\n",
    "        downsample(128, 4,),\n",
    "        downsample(256, 4,),\n",
    "        downsample(512, 4,),\n",
    "        downsample(512, 4,),\n",
    "    ]\n",
    "    up_sampling = [\n",
    "        upsampling(512, 4, dropout=True),\n",
    "        upsampling(512, 4, dropout=True),\n",
    "        upsampling(256, 4, dropout=False),\n",
    "        upsampling(128, 4, dropout=False),\n",
    "        upsampling(64, 4, dropout=False),\n",
    "    ]\n",
    "    \n",
    "    initializer = tf.random_normal_initializer(0, .02)\n",
    "    last_layer = keras.layers.Conv2DTranspose(filters=3, kernel_size=4, strides=2, padding=\"same\")\n",
    "    output_image = keras.layers.Activation(\"tanh\")\n",
    "    \n",
    "    x = inputs\n",
    "    skips = []\n",
    "    for down in down_sampling:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "\n",
    "    skips = reversed(skips[:-1])\n",
    "    for up, skip in zip(up_sampling, skips):\n",
    "        x = up(x)\n",
    "        x = keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "    x = output_image(last_layer(x))\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f6c8b",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65f9fe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "    initializer = tf.random_normal_initializer(0., .02)\n",
    "    gamma_init = keras.initializers.RandomNormal(mean=.0, stddev=.02)\n",
    "\n",
    "    inp = keras.layers.Input(shape=[256, 256, 3], name='input_image')\n",
    "\n",
    "    x = inp\n",
    "    down1 = downsample(64, 4, False)(x) \n",
    "    down2 = downsample(128, 4)(down1) \n",
    "    down3 = downsample(256, 4)(down2)\n",
    "\n",
    "    zero_pad1 = keras.layers.ZeroPadding2D()(down3)\n",
    "    conv = keras.layers.Conv2D(512, 4, strides=1,\n",
    "                         kernel_initializer=initializer,\n",
    "                         use_bias=False)(zero_pad1) \n",
    "\n",
    "    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n",
    "\n",
    "    leaky_relu = keras.layers.LeakyReLU()(norm1)\n",
    "\n",
    "    zero_pad2 = keras.layers.ZeroPadding2D()(leaky_relu) \n",
    "\n",
    "    last = keras.layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(zero_pad2)\n",
    "\n",
    "    return keras.Model(inputs=inp, outputs=last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17acd6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "monet_generator = Generator() \n",
    "photo_generator = Generator() \n",
    "\n",
    "monet_discriminator = Discriminator() \n",
    "photo_discriminator = Discriminator() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e326c1",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4980e6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGAN(keras.Model):\n",
    "    def __init__(self, monet_generator, photo_generator,\n",
    "                 monet_discriminator, photo_discriminator,\n",
    "                 lambda_cycle=10):\n",
    "        super(CycleGAN, self).__init__()\n",
    "        self.monet_generator = monet_generator\n",
    "        self.photo_generator = photo_generator\n",
    "        self.monet_discriminator = monet_discriminator\n",
    "        self.photo_discriminator = photo_discriminator\n",
    "        self.lambda_cycle = lambda_cycle\n",
    "        \n",
    "    def compile(self, monet_gen_optimizer, photo_gen_optimizer,\n",
    "                monet_disc_optimizer, photo_disc_optimizer,\n",
    "                generator_loss_fn, discriminator_loss_fn,\n",
    "                cycle_loss_fn, identity_loss_fn):\n",
    "        super(CycleGAN, self).compile()\n",
    "        self.monet_gen_optimizer = monet_gen_optimizer\n",
    "        self.photo_gen_optimizer = photo_gen_optimizer\n",
    "        self.monet_disc_optimizer = monet_disc_optimizer\n",
    "        self.photo_disc_optimizer = photo_disc_optimizer\n",
    "        self.generator_loss_fn = generator_loss_fn\n",
    "        self.discriminator_loss_fn = discriminator_loss_fn\n",
    "        self.cycle_loss_fn = cycle_loss_fn\n",
    "        self.identity_loss_fn = identity_loss_fn\n",
    "        \n",
    "    def train_step(self, batch_data):\n",
    "        real_monet, real_photo = batch_data\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Photo to Monet back to Photo\n",
    "            fake_monet = self.monet_generator(real_photo, training=True)\n",
    "            cycled_photo = self.photo_generator(fake_monet, training=True)\n",
    "\n",
    "            # Monet to Photo back to Monet\n",
    "            fake_photo = self.photo_generator(real_monet, training=True)\n",
    "            cycled_monet = self.monet_generator(fake_photo, training=True)\n",
    "\n",
    "            # Generating itself\n",
    "            same_monet = self.monet_generator(real_monet, training=True)\n",
    "            same_photo = self.photo_generator(real_photo, training=True)\n",
    "\n",
    "            # Discriminator for real images\n",
    "            disc_real_monet = self.monet_discriminator(real_monet, training=True)\n",
    "            disc_real_photo = self.photo_discriminator(real_photo, training=True)\n",
    "\n",
    "            # Discriminator for fake images\n",
    "            disc_fake_monet = self.monet_discriminator(fake_monet, training=True)\n",
    "            disc_fake_photo = self.photo_discriminator(fake_photo, training=True)\n",
    "\n",
    "            # Generator loss\n",
    "            monet_gen_loss = self.generator_loss_fn(disc_fake_monet)\n",
    "            photo_gen_loss = self.generator_loss_fn(disc_fake_photo)\n",
    "\n",
    "            # Total cycle consistency loss\n",
    "            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n",
    "\n",
    "            # Total generator loss\n",
    "            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle)\n",
    "            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n",
    "\n",
    "            # Discriminator loss\n",
    "            monet_disc_loss = self.discriminator_loss_fn(disc_real_monet, disc_fake_monet)\n",
    "            photo_disc_loss = self.discriminator_loss_fn(disc_real_photo, disc_fake_photo)\n",
    "\n",
    "        # Calculate gradients\n",
    "        monet_gen_gradients = tape.gradient(total_monet_gen_loss, self.monet_generator.trainable_variables)\n",
    "        photo_gen_gradients = tape.gradient(total_photo_gen_loss, self.photo_generator.trainable_variables)\n",
    "        monet_disc_gradients = tape.gradient(monet_disc_loss, self.monet_discriminator.trainable_variables)\n",
    "        photo_disc_gradients = tape.gradient(photo_disc_loss, self.photo_discriminator.trainable_variables)\n",
    "\n",
    "        # Apply gradients\n",
    "        self.monet_gen_optimizer.apply_gradients(zip(monet_gen_gradients, self.monet_generator.trainable_variables))\n",
    "        self.photo_gen_optimizer.apply_gradients(zip(photo_gen_gradients, self.photo_generator.trainable_variables))\n",
    "        self.monet_disc_optimizer.apply_gradients(zip(monet_disc_gradients, self.monet_discriminator.trainable_variables))\n",
    "        self.photo_disc_optimizer.apply_gradients(zip(photo_disc_gradients, self.photo_discriminator.trainable_variables))\n",
    "        \n",
    "        return {\n",
    "            \"monet_gen_loss\": total_monet_gen_loss,\n",
    "            \"photo_gen_loss\": total_photo_gen_loss,\n",
    "            \"monet_disc_loss\": monet_disc_loss,\n",
    "            \"photo_disc_loss\": photo_disc_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c205c4db",
   "metadata": {},
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27b988c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "def discriminator_loss(real, generated):\n",
    "    real_loss = bce_loss(tf.ones_like(real), real)\n",
    "    generated_loss = bce_loss(tf.zeros_like(generated), generated)\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    return total_disc_loss * 0.5\n",
    "\n",
    "def generator_loss(generated):\n",
    "    return bce_loss(tf.ones_like(generated), generated)\n",
    "\n",
    "def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n",
    "    loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "    return LAMBDA * loss1\n",
    "\n",
    "def identity_loss(real_image, same_image, LAMBDA):\n",
    "    loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "    return LAMBDA * 0.5 * loss\n",
    "\n",
    "# Apply strategy.scope() to the defined loss functions\n",
    "with strategy.scope():\n",
    "    discriminator_loss = tf.function(discriminator_loss)\n",
    "    generator_loss = tf.function(generator_loss)\n",
    "    calc_cycle_loss = tf.function(calc_cycle_loss)\n",
    "    identity_loss = tf.function(identity_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b1b876d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Graph execution error:\n\nDetected at node 'IteratorGetNext' defined at (most recent call last):\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n      handle._run()\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2914, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Temp/ipykernel_10328/919252902.py\", line 27, in <module>\n      cycle_gan_model.fit(\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1009, in step_function\n      data = next(iterator)\nNode: 'IteratorGetNext'\nNewRandomAccessFile failed to Create/Open: monet00-60.tfrec : The system cannot find the file specified.\r\n; No such file or directory\n\t [[{{node IteratorGetNext}}]] [Op:__inference_train_function_132122]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10328/919252902.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m cycle_gan_model.fit(\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonet_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphoto_ds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Graph execution error:\n\nDetected at node 'IteratorGetNext' defined at (most recent call last):\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n      handle._run()\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2914, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Temp/ipykernel_10328/919252902.py\", line 27, in <module>\n      cycle_gan_model.fit(\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Vedhik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1009, in step_function\n      data = next(iterator)\nNode: 'IteratorGetNext'\nNewRandomAccessFile failed to Create/Open: monet00-60.tfrec : The system cannot find the file specified.\r\n; No such file or directory\n\t [[{{node IteratorGetNext}}]] [Op:__inference_train_function_132122]"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    monet_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "    photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "    monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "    photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "with strategy.scope():\n",
    "    cycle_gan_model = CycleGan(\n",
    "        monet_generator, photo_generator, monet_discriminator, photo_discriminator\n",
    "    )\n",
    "\n",
    "    cycle_gan_model.compile(\n",
    "        m_gen_optimizer = monet_generator_optimizer,\n",
    "        p_gen_optimizer = photo_generator_optimizer,\n",
    "        m_disc_optimizer = monet_discriminator_optimizer,\n",
    "        p_disc_optimizer = photo_discriminator_optimizer,\n",
    "        gen_loss_fn = generator_loss,\n",
    "        disc_loss_fn = discriminator_loss,\n",
    "        cycle_loss_fn = calc_cycle_loss,\n",
    "        identity_loss_fn = identity_loss\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "cycle_gan_model.fit(\n",
    "    tf.data.Dataset.zip((monet_ds, photo_ds)),\n",
    "    epochs=25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e23389",
   "metadata": {},
   "source": [
    "# Discussion/Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c2690f",
   "metadata": {},
   "source": [
    "I've built the generator, discriminator, and cyclic gan model with the loss functions; howvewr there is an error when fitting the model. Due to this, I'm unable to have a proper discussion and results. In future iterations, The epochs can be experimented with to see how it affects the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
